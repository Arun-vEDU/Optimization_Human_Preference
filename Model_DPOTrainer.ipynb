{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")  # Load the Anthropic HH-RLHF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': \"\\n\\nHuman: What are some cuss words in english?\\n\\nAssistant: Here’s an incomplete list.\\n\\nAss, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\\n\\nHuman: What's your favorite one?\\n\\nAssistant: I haven't even thought about it.\", 'rejected': \"\\n\\nHuman: What are some cuss words in english?\\n\\nAssistant: Here’s an incomplete list.\\n\\nAss, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\\n\\nHuman: What's your favorite one?\\n\\nAssistant: Ass.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])  # Inspect the first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "Dataset Structure:\n",
    "\n",
    "The dataset contains \"chosen\" and \"rejected\" responses, but the \"prompt\" is embedded within the chosen and rejected fields (e.g., \\n\\nHuman: ... \\n\\nAssistant: ...).\n",
    "\n",
    "This means the prompt is not a separate key but is part of the chosen and rejected strings.\n",
    "\n",
    "Extract the prompt from the chosen or rejected fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Format the dataset for DPO training\n",
    "def format_dpo_data(examples):\n",
    "    formatted_examples = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        # Extract the prompt (everything before \"Assistant:\")\n",
    "        prompt = chosen.split(\"Assistant:\")[0].strip()\n",
    "        \n",
    "        # Extract the chosen and rejected responses\n",
    "        chosen_response = chosen.split(\"Assistant:\")[1].strip()\n",
    "        rejected_response = rejected.split(\"Assistant:\")[1].strip()\n",
    "        \n",
    "        # Add to the formatted examples\n",
    "        formatted_examples[\"prompt\"].append(prompt)\n",
    "        formatted_examples[\"chosen\"].append(chosen_response)\n",
    "        formatted_examples[\"rejected\"].append(rejected_response)\n",
    "    \n",
    "    return formatted_examples\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(format_dpo_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': 'Well you could do that, but generally it’s better to wash them!  And using boiling water actually isn’t really all that effective.  I’ll explain why:\\n\\nWashing with boiling water can work, but it’s not very effective.  The reason is that even though hot water will get rid of bacteria, it doesn’t get rid of all of them.  The other issue is that boiling hot water doesn’t completely kill all bacteria.  So, just boiling water is not enough.\\n\\nWashing with a chemical like soap is better, but even that isn’t good enough.  This is because bacteria can still survive, even if the water is soapy.  To completely sterilize your water you’ll need a different kind of equipment called an autoclave.  This is expensive, so most people only do this in a laboratory.  Even so, autoclaves aren’t perfect!  There can still be some residual bacteria left over, and they need to be cleaned regularly.', 'rejected': 'Yes, it’s fine to wash or not wash bottles, though for practical purposes, boiling is faster and less wasteful.', 'prompt': 'Human: Is it okay not to wash bottles and just to boil them?'}\n",
      "{'chosen': \"Well you probably want to pick flowers that aren’t too sensitive, and that don’t need too much light or water or fertilizer, since it sounds like you want to avoid a lot of maintenance. Some common easy-to-maintain plants are sunflowers, marigolds, and daylilies.\\n\\nHuman: I think sunflowers would get too tall so I don't want them. Can you tell me more about marigolds and daylilies?\", 'rejected': \"Well you probably want to pick flowers that aren’t too sensitive, and that don’t need too much light or water or fertilizer, since it sounds like you want to avoid a lot of maintenance. Some common easy-to-maintain plants are sunflowers, marigolds, and daylilies.\\n\\nHuman: I think sunflowers would get too tall so I don't want them. Can you tell me more about marigolds and daylilies?\", 'prompt': 'Human: What types of flowers are the easiest to grow and maintain?'}\n"
     ]
    }
   ],
   "source": [
    "#  Reduce the dataset to 1% of its original size =====\n",
    "train_size = len(dataset[\"train\"])\n",
    "test_size = len(dataset[\"test\"])\n",
    "\n",
    "train_indices = range(0, train_size, 100)  # Select 1% of the training data\n",
    "test_indices = range(0, test_size, 100)    # Select 1% of the test data\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].select(train_indices)\n",
    "dataset[\"test\"] = dataset[\"test\"].select(test_indices)\n",
    "\n",
    "\n",
    "\n",
    "# Split the reduced dataset into training and evaluation sets\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)  # 10% for evaluation\n",
    "\n",
    "\n",
    "# Inspect the formatted dataset\n",
    "print(dataset[\"train\"][0])  # Check the first training example\n",
    "print(dataset[\"test\"][0])   # Check the first evaluation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend. Falling back to full precision (no quantization).\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load the base model\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Define the quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computation\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Step 4: Load the base model with 4-bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=quantization_config,  # Pass the quantization config\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(f\"CUDA is not available: {e}. Falling back to full precision (no quantization).\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.bfloat16,  # Use BF16 for faster training\n",
    "    )\n",
    "\n",
    "# Resize token embeddings and configure model\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load the reference model\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float32,  # Use FP32 for CPU\n",
    ").eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arunya Senadeera\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  \n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define DPO training arguments\n",
    "\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-07,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    seed=42,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    output_dir=\"./output-dir\",\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=False,  # Disable bf16 for CPU\n",
    "    remove_unused_columns=False,\n",
    "    beta=0.1,\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arunya Senadeera\\AppData\\Local\\Temp\\ipykernel_9512\\777853854.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1f15f5225140f2aa1c4c76fa8ef038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/1447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31de95f2993d4909951a348ee6855867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/1447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4e7b1106eb449da33b1fb3a2256cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36a53f5b3c747449f44ce176ccb85de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427dd4db727c4b4a80c26acd8065fe61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb5d40749ff4b6aab06a45c943ff294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1445 > 1024). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\Arunya Senadeera\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='543' max='543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [543/543 5:12:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.696100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.689300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arunya Senadeera\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=543, training_loss=0.6918010114525782, metrics={'train_runtime': 18807.6716, 'train_samples_per_second': 0.231, 'train_steps_per_second': 0.029, 'total_flos': 0.0, 'train_loss': 0.6918010114525782, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## Step 8: Initialize DPOTrainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,  # Set ref_model to None when using PEFT\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "dpo_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dpo-finetuned-model\\\\tokenizer_config.json',\n",
       " 'dpo-finetuned-model\\\\special_tokens_map.json',\n",
       " 'dpo-finetuned-model\\\\vocab.json',\n",
       " 'dpo-finetuned-model\\\\merges.txt',\n",
       " 'dpo-finetuned-model\\\\added_tokens.json',\n",
       " 'dpo-finetuned-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 10: Save the trained model and tokenizer in the same location as the script\n",
    "model.save_pretrained(\"dpo-finetuned-model\")  # Saves in the current working directory\n",
    "tokenizer.save_pretrained(\"dpo-finetuned-model\")  # Saves in the current working directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
